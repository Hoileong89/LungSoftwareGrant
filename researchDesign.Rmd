## __3(c) Research design__

### 3(c.1) Preliminary data

### 3(c.1.1) Generic ANTs core tools for image analysis and processing

Many of the core programs comprising portions of the proposed pulmonary software framework
have been created and made available within ANTs (and either simultaneously or subsequently
made available in ITK).  However, as mentioned earlier, these programs have more
general application and require pulmonary-specific tuning for the tasks targeted by
this proposal.  The following list comprises these core software tools for
tuning, subsequent extensions, documentation, tutorial generation, and the creation of
easy-to-use bash scripts for large-scale processing of pulmonary imaging data:

![Core processing tools that have made the ANTs package one of the most popular neuroimaging
toolkits.  Fundamental processing tasks such as image registration, template generation,
bias correction, denoising, intensity-based segmentation, and joint label
fusion are extremely well-performing software components which have been utilized for
neuroimaging tasks such as brain extraction and cortical thickness estimation.
The target applications of these core tools have an immediate analog for lung-specific
tasks such as lung and lobe segmentation.](Figs/coreANtsToolsNeuro.png)

__ANTs image registration.__  One of the most important methodological developments in
medical image analysis is the advent of image registration techniques capable of
accommodating the highly complex inter-individual variations seen in human anatomy.
Our team is well-recognized for seminal contributions to the field that date back to the
original elastic matching method of Bajcsy and co-investigators
[@Bajcsy:1982aa;@Bajcsy:1989aa;@Gee:1993aa]. Our most recent work, embodied in the ANTs
open-source, cross-platform toolkit for multiple modality image processing,
continues to set the standard in the field. ANTs not only encodes the most advanced results
in registration research, notably the Symmetric Normalization (SyN) algorithm for
diffeomorphisms [@Avants:2008aa], but also packages these within a full featured platform that
includes an extensive library of similarity measures, transformation types, and regularizers.
Recently, a thorough comparison with the original SyN algorithm was performed using a B-spline variant [@Tustison:2013ac].
This evaluation utilized multiple publicly available, annotated brain data sets and
demonstrated statistically significant improvement in label overlap measures.  As part of
that study, we produced the scripts ``antsRegistrationSyN.sh`` and
``antsRegistrationSyNQuick.sh`` which provide a simple interface to our normalization tools
for brain-specific normalization.  _Similar to the developments that we are proposing,
these scripts were modified to serve as a follow-up  entry into the EMPIRE10 lung registration
challenge where B-spline SyN performed better than its original counterpart on pulmonary
data [@Tustison:2012aa]._


__Multi-modal template generation.__  Given the variability in anatomical shape across
populations and the lack of publicly available atlases for specific organs, generating population- or
subject-specific optimal shape/intensity templates significantly enhances study potential
[@Avants:2010aa;@Tustison:2014aa].  First, an average template is estimated via a voxel-wise
mean of all the individual subject
images.  This estimate is iteratively updated by registering each image to the current
template, performing a voxelwise average to create a new estimate, and then "reshaping"
this template based on the average inverse transformation which "moves" the template estimate
closer to the group mean.  See Figure 2 for a cohort-specific multi-modal brain template
for females in the age range 50--60.
This functionality has proven to be a vital component of the ANTs
toolkit for performing neuroimaging research
(e.g., [@Avants:2015aa;@Datta:2012aa;@McMillan:2014aa;@Cook:2014aa;@Tustison2014ad;@Tustison:2014ab]).


__Bayesian segmentation with spatial and MRF priors.__
Early statistically-based segmentation work appropriated NASA satellite image processing
software for classification of head tissues in 2-D MR images [@Vannier:1985aa]. Following
this work, many researchers adopted statistical methods for $n$-tissue anatomical brain
segmentation. The Expectation-Maximization (EM) framework is natural [@Dempster:1977aa]
given the "missing data" aspect of this problem. The work described in [@Wells:1996aa]
was one of the first to use EM for finding a locally optimal solution by iterating between
bias field estimation and tissue segmentation.  Core components of this type of work is
the explicit modeling of the tissue intensity values as statistical distributions
[@Cline:1990aa;@Kikinis:1992aa] and the use of MRF modeling [@Geman:1984aa] for regularizing
the classification results [@Held:1997aa].  Recently, reseachers have begun to rely on spatial
prior probability maps of anatomical structures of interest to encode domain knowledge
[@Van-Leemput:1999aa;@Ashburner:2005aa] by providing spatial
prior probability maps and an initial segmentation.  Although this particular segmentation
framework has significant application in the neuroimaging domain, it has also applicable
to other domains such as breast MRI [@Gubern-Merida:2015aa;@Ribes:2014aa] and functional
ventilation of the lung [@Tustison:2011aa].  However, despite the numerous algorithms and other developments which have been proposed
over the years, there are an extremely limited number of software implementations to
perform these types of segmentations.
This deficit inspired us to create our own Bayesian
segmentation framework [@Avants:2011aa] (denoted as Atropos) which we have made publicly
available within ANTs.

__N4 bias correction.__  Critical to quantitative processing of MRI is the minimization of field
inhomogeneity effects which produce artificial low frequency intensity variation across the image.
Large-scale studies, such as ADNI, employ perhaps the most widely used bias correction algorithm,
N3 [@Sled:1998aa], as part of their standard protocol [@Boyes:2008aa].
In [@Tustison:2010aa] we introduced an improvement of N3, denoted as "N4", which demonstrates
a significant increase in performance and convergence behavior on a variety of data. This
improvement is a result of an enhanced fitting routine (which includes multi-resolution
capabilities) and a modified optimization formulation.

__Joint label fusion for prior-based segmentation.__  Joint label fusion (JLF) is the current
state-of-the-art for propagating expert labelings from a reference atlas library onto new
instances of unlabeled data. Image registration is used to align the atlas library
(images + segmentations) to a common space. A statistical model is then used to combine
the "guesses" from all the normalized atlas labels to provide a "best guess" estimate of
the target labeling. Several such algorithms have been developed and much effort has been
devoted to determining relative performance levels. See, for example, the recent MICCAI 2012
Grand Challenge and Workshop on Multi-Atlas Labeling).  The joint fusion (JLF) algorithm of
[@Wang:2013aa;@Wang:2013ab] is one of the top performing JLF algorithms. JLF is capable
of predicting anatomical labels with accuracy that rivals expert anatomists [@Yushkevich:2010aa].
It has proven its effectiveness in lung [@Tustison:2015aa], cardiac data [@MALF],
the human brain [@Tustison:2014ab], and in multiple modality canine MRI [@MALF].


__Spatially adaptive denoising.__ Patch-based
denoising is critical for data "cleaning" prior to subsequent processing such
as segmentation or spatial normalization.  ANTs implements a state-of-the-art
spatially adaptive version to denoising recently proposed in [@Manjon:2010aa].

The previously described core tools, as well as several others, have been part of ANTs and
ITK development efforts for more than a decade.  It was precisely the deficiency of publicly
available tools within the neuroscience community that motivated the inception and
continued development of ANTs.  As a result, our team is well-recognized for our many
open-source contributions including important contributions to the
field of image registration outlined earlier.  Indeed, ANTs-based image registration serves
as the basis for the registration component of the latest version of the National Library of
Medicine Insight Toolkit (ITK) programming library (http://www.itk.org). The
combination of state-of-the-art algorithms and feature-rich flexibility has translated
to top-placed rankings in major independent evaluations for core elements of the ANTs
toolkit:

* SyN was a top performer in a fairly recent large-scale brain normalization evaluation [@Klein:2009aa].
* SyN also competed in the Evaluation of Methods for Pulmonary Image
REgistration 2010 (EMPIRE10) challenge [@Murphy:2011aa] where it was the top performer
for the benchmarks
used to assess lung registration accuracy and biological plausibility of the inferred
transform (i.e., boundary alignment, fissure alignment, landmark correspondence, and
displacement field topology).  Even though the competition has continued to the present,
SyN has remained top-ranked.
* The joint label fusion algorithm of [@Wang:2012aa;@Wang:2013aa] (coupled with SyN)
  was top-ranked in the MICCAI 2012 challenge for labeled brain data [@Landman2012]
  and in 2013 for labeled canine hind leg data [@Asman2013].
* The multivariate template capabilities in ANTs were combined with random forests to win
the Brain Tumor segmentation (BRATS) competition at MICCAI 2013 [@Tustison:2014aa].
* A B-spline variant of the SyN algorithm [@Tustison:2013ac] won the best paper award at the
STACOM 2014 workshop for cardiac motion estimation [@Tustison:2015ab].

### 3(c.1.2) Neuroimaging ANTs use as a model for the pulmonary community

ANTs takes advantage of
the mature Insight ToolKit in providing an optimal software framework for building scripts
and programs specifically for neuroimaging.  For example, the following core neuroimage
processing algorithms have been made available through our ANTs toolkit (complete with
online self-contained examples with developer-tuned parameters) and have been used extensively by our group and
others:

* brain normalization [@Avants:2011ab;@Avants:2014aa] (https://github.com/stnava/BasicBrainMapping),
* brain template generation [@Avants:2010aa] (https://github.com/ntustison/TemplateBuildingExample),
* skull-stripping or brain extraction [@Avants:2010ab;@Tustison:2014ab] (https://github.com/ntustison/antsBrainExtractionExample),
* prior-based brain tissue segmentation [@Avants:2011ab] (https://github.com/ntustison/antsAtroposN4Example),
* cortical  thickness estimation [@Das:2009aa;@Tustison:2014ab] (https://github.com/ntustison/antsCorticalThicknessExample),
* brain tumor segmentation [@Tustison:2014aa] (https://github.com/ntustison/ANTsAndArboles), and
* cortical labeling [@Wang:2012aa;@Wang:2013aa] (https://github.com/ntustison/MalfLabelingExample).

All of these tools have been wrapped in easy-to-use, well-documented shell scripts.  For
example, the ANTs cortical thickness pipeline, as outlined in [@Tustison:2014ab], comprises
four major steps:  (1) bias correction, (2) brain extraction, (3) $n$-tissue segmentation,
and (4) cortical thickness estimation.  Each step requires its own set of ANTs tools with
appropriately tuned parameters. To maximize the utility of the pipeline for the interested
user, in [@Tustison:2014ab] we provide all the necessary programs (properly tuned) with
a minimal set of input data required to obtain good results for common data.  The result
is an easy-to-use script that can be invoked by the programmer and non-programmer alike to
obtain the desired processed data which outperforms the current state-of-the-art.  This is
an example command call for the ANTs cortical thickness pipeline:
```bash
  # ANTs processing call for a single subject

  $ sh antsCorticalThickness.sh -d 3 \
                                -a IXI/T1/IXI002-Guys-0828-T1.nii.gz \
                                -e IXI/template/T_Template0.nii.gz \
                                -m IXI/template/T_template0ProbabilityMask.nii.gz \
                                -f IXI/template/T_template0ExtractionMask.nii.gz \
                                -p IXI/template/Priors/priors%d.nii.gz \
                                -o IXI/ANTsResults/IXI002-Guys-02828-
```
This approach to reducing the steep learning curve associated with many processing pipelines
has several benefits.  Bash is an extremely common command language that permits large-scale
processing.  Thus, running several jobs on a cluster infrastructure is straightforward with
this approach (as opposed to a GUI-driven processing paradigm).  Such scripts are readable
by the interested user who can glean parameters as well as manually make changes.


### 3(c.1.3) ITK-SNAP

Project investigator Paul Yushkevich leads the development of ITK-SNAP [@Yushkevich:2006aa], a multi-platform
open-source tool for interactive user-guided medical image segmentation. ITK-SNAP provides
an effective combination of semi-automatic segmentation functionality based on active
contours [@Caselles:1997aa;@Zhu:1996aa] and manual delineation functionality, put together into a compact and
easy-to-learn graphical user interface. ITK-SNAP supports segmentation of multiple
volumetric imaging modalities, species, and anatomical regions, without bias to any
particular problem domain. Compared to other, larger open-source image analysis tools,
ITK-SNAP design focuses specifically on the problem of image segmentation, and extraneous
or unrelated features are kept to a minimum. The design also emphasizes interaction and
ease of use, with the bulk of the development effort dedi- cated to the user interface.
ITK-SNAP has thousands of users (there have been over 2000 downloads per month in the
last year), and our 2006 paper on ITK-SNAP [@Yushkevich:2006aa] has been cited over 1400 times (Google
Scholar) in the context of various biomedical domains. In recent years, ITK-SNAP
development and maintenance were funded by grant 5R01 EB014346, and under this grant,
powerful new functionality for registration was developed. ITK-SNAP will be used in this project for manual labeling
of the proposed brain atlases; it is already used for this purpose by many investigators,
including our partners at Duke CIVM and the Allen Institute annotation team responsible
for labeling its new Common Coordinate Framework.
 Most crucially, we believe that our track record with ITK-SNAP as
well as ANTs demonstrates our team’s commitment to producing high-quality research
software and making it accessible to the wider research community through open-source
practices, intuitive user interfaces, and outreach efforts. These strengths of the team
will be applied to the software and data developed in the course of this project.





























### 3(c.2) __Specific Aim 1:__   To develop a set of open-source software tools for CT, proton, and 3He pulmonary computational analysis.

Analogous to the neuroimaging tasks described earlier, several algorithmic categories exist for lung image analysis which, as we have
stated previously, do not exist in any comprehensive, publicly available package.  This is
in spite of the fact that new algorithms for lung image analysis are frequently reported
in the literature.   An extensive survey concentrating on the years 1999–2004 is given in
[@Sluimer:2006aa] which
covers computer-aided diagnosis of lung disease and lung cancer in CT (i.e., detection and
tracking of
pulmonary nodules) and provides an overview of the many relevant segmentation methods for
pulmonary structures. Although many algorithms existed at the time, continued technical
development has only increased the number of available algorithms.    However, despite the
continued _reporting_ of pulmonary image analysis algorithms, there is no corresponding
increase in algorithmic _availability_.

_A primary assumption for this proposal is that, through extension and continued development
of ANTs and ITK functionality, we can make a significant impact for the pulmonary imaging
research community in both basic science and clinical workflows by developing lung-specific
algorithms which are easy to use as we have done for the neuroimaging community._  The
following is a small sampling of more recently reported techniques for CT analysis that
would be incorporated into ITK-Lung:

* whole lung differentiation from the chest wall (e.g., [@De-Nunzio:2011aa;@Prasad:2008aa;@Wang:2009aa;@Rikxoort:2009aa]),
* bronchial structure extraction (e.g., [@Zheng:2007aa;@Nakamura:2008aa] and the many submissions to the recent Extraction of Airways from CT (ExACT) challenge of the 2nd International Workshop on Pulmonary Image Analysis [@Lo:2009aa]),
* vasculature segmentation (e.g., [@Agam:2005aa;@Korfiatis:2011aa]),
* lobe and/or fissure detection (e.g., [@Qi:2014aa;@Doel:2015aa]),
* feature extraction and classification (e.g., [@Uppaluri:1999aa;@Rosas:2011aa;@DeBoer:2014aa]), and
* nodule detection (e.g., [@Messay:2010aa] and the many submissions to the Automatic Nodule Detection (ANODE09) challenge of the 2009 CAD Conference of SPIE Medical Imaging [@Ginneken:2010aa]).

Although this list is restricted to CT image analysis, inclusion of additional techniques
specific to other modalities has additional benefit and are included in
this proposal.  Using ANTs core tools, we have produced several lung-specific algorithms
for core tasks:

__Atlas-based lung segmentation.__  Identification of anatomical structure in MRI is often a
crucial preprocessing step for quantification of morphological features or ventilation
information from functional images.  Quantitative regional analysis often requires the
identification of lung and lobar anatomy.  Although much algorithmic research for lung
segmentation has been
reported in the CT literature [@Rikxoort:2013aa], co-opting such technologies is complicated by
MRI-specific issues such as RF coil inhomogeneity, presence and resolution of structural detail, and
the absence of a physically-based intensity scaling.

We recently proposed a multi-atlas approach for automatically segmenting the left and right
lungs in proton MRI [@Tustison:2015aa].  Multi-atlas approaches to segmentation have proven highly
successful in neuroimaging [@Wang:2012aa;@Wang:2013aa] and these methods translate readily to
the pulmonary domain.  Wherease many current strategies for lung image segmentation employ
low-level processing techniques based on encodable heuristics, consensus-based strategies,
in contrast, optimize the prior knowledge applied to a specific segmentation problem (cf Figure 3).
The evaluation of our proposed method [@Tustison:2015aa] demonstrated good performance
with Jaccard overlap measures for the left and right lungs being $0.966 \pm 0.018$ and
$0.970 \pm 0.016$, respectively.  One of the benefits of this approach is that can also
be applied effectively to pulmonary CT.

![Sample lung and lobe estimation results in both proton MRI and CT using our
atlas-based strategy.  (Left) Lung segmentation and lobe estimation results for the given
proton MRI.  Although lobe estimation is dependent solely on the warped atlases, we are
able to obtain accurate estimates of lobes which are useful for more regional analysis
and provide a more intuitive and universal subdivision of the lungs than previous partitioning
schemes.  (Right) The utility of this method extends to CT where the integrity of lobar anatomical
markers (such as the lack of fissures illustrated by the red arrows) have been compromised due to
disease.](Figs/lungEstimation.png)

__Atlas-based lobe estimation.__  For regional investigation of certain lung pathologies and
conditions, it is often useful to quantify measurements of interest within more localized
regions, such as the lobes.  However there is little (if any) usable information in proton
MRI for image-based lobar segmentation which has led to alternative geometric subdivisions
which are ad hoc, non-anatomical, and do not adequately address intra- and inter-subject
correspondences.  However, we can take advantage of
inter-subject similarities in lobar geometry to provide a prior-based estimation of
lobar divisions using a consensus labeling approach (cf Figure 3).

To generate the lobe segmentation in a target proton or CT lung image, we first generate the
binary whole lung mask using the whole lung atlas-based estimation.  We
then register the set of CT lung masks which have beeen expertly annotated to the target
binary lung mask using the B-spline
SyN registration approach described earlier [@Tustison:2013ac].
 Subsequently, we warp the set of CT lobe labels to the target image using
the CT mask-to-target mask transformation.  Since we have no intensity information inside
the target lung mask and CT atlas lung masks, we use a simply majority voting strategy to
generate the optimal labeling for the target image.  Following the majority voting, we
remove any labelings outside the lung mask and assign any unlabeled voxels with the label
closest in distance to that voxel.  This methodology is more thoroughly described in
[@Tustison:2015aa] where we showed that lobar overlap measures in proton MRI were on par
with the  state-of-the-art CT methods where fissure information is actually visible
(left upper: $0.882 \pm 0.059$, left lower: $0.868 \pm 0.06$, right upper: $0.852 \pm 0.067$,
right middle: $0.657 \pm 0.130$, right lower: $0.873 \pm 0.063$).

__Ventilation quantification.__
Automated or semiautomated approaches for classifying areas of varying degrees of ventilation
are of potential benefit for pulmonary functional analysis.  In [@Tustison:2011aa], we presented
an automated algorithmic pipeline for ventilation-based partitioning of the lungs in hyperpolarized
3He and 129Xe MRI.  Without ground truth data for evaluation, we used a consensus labeling approach [@Warfield:2004aa]
to simultaneously estimate the true segmentation from given ``raters''
which included the segmentation from our automated approach and the manual tracings of three trained
individuals.  In terms of combined specificity and sensitivity, our automated algorithm
demonstrated superior performance with the added benefit of being reproducible and less
time-consuming.

Since the initial development, we have continued to improve this segmentation pipeline by
incorporating an iterative bias-correction/segmetnation estimation scheme.   An additional
component that improves results is an ANTs-based implementation of the patch-based denoising protocol
described in [@Manjon:2010aa].  Example longitudinal segmentation results are provided
in Figure 4.

![Pulmonary functional segmentation using the algorithmic framework first described in [@Tustison:2011aa]
for hyperpolarized 3He MRI.  These data were taken from a current study looking at the
implications in ventilation pre- and post-albuterol intake including an additional
acquisition at some delay period following the post-albuterol imaging.  The
ventilation-based segmentation is as follows:  red = no ventilation, green = poorly
ventilated, blue = normally ventilated, and yellow = well-ventilated.
Note the improvement in both the qualitative assessment of the ventilation map (top) and the corresponding
segmentation time course (bottom) followed by an approximate return to pre-albuterol
conditions following the delay period.](Figs/prePostAlbuterol.png)





<!--
Crucial to the development of ITK-Lung will be domain-specific experience.  For example,
although ANTs performance in brain
registration has been independently evaluated and found to be of relatively high quality
[@Klein:2009aa], tailoring our registration tool to achieve top performance levels for the
EMPIRE10 challenge (Evaluation of Methods
for Pulmonary Image REgistration 2010) required significant empirically-based tuning.  In
addition, new innovations in diffeomorphic registration technology has led to a Symmetric
Normalization B-spline variant which has demonstrated accurate normalizations
[@Tustison:2013ac]
and transformations which are particularly well-suited for pulmonary data [@Tustison:2012aa].
-->

__Multi-modal lung template construction.__
Additionally, although the template construction algorithm described in [@Avants:2010aa] is,
as pointed out earlier, frequently applied to T1-weighted brain data,
it is sufficiently generic such that it can also be applied
to pulmonary data.  Also, new innovations in diffeomorphic registration technology has led to a Symmetric
Normalization B-spline variant which has demonstrated accurate normalizations
[@Tustison:2013ac] and transformations which are particularly well-suited for pulmonary data [@Tustison:2012aa].

In Figure 4, we illustrate the major processing components of a recent study
analyzing local changes based on a pulmonary treatment plan [@Tustison:2013ad].  This
study employed several of the tools we are proposing for inclusion in the specified aims.
The first major component is the construction of a single-subject 3He/proton MRI
template for all five imaging time points.  This step generates the statistical coordinate
system for the voxelwise regression analysis of the normalized intensities to determine
correlation with expected treatment effects.

![Voxelwise regression analysis to determine image-based response to treatment.  First,
a multi-modal, single-subject template is created to bring all time point images to
the same coordinate system.  4-D segmentation is performed on the longitudinal time series
of 3-D image volumes.  Treatment
effects are expected to follow the simplified treatment hypothesis illustrated with the
dashed blue line in the plot on the right.  To explore how the longitudinal change in expected
ventilation follows this treatment hypothesis with image data, we smooth the aligned expected
ventilation maps (to account for potential voxelwise misalignments) and then quantify how the
voxelwise intensities regress with the simplified treatment hypothesis.  This quantification
is visualized using the correlation maps depicted in the template space (top right).  Positive
correlations with the expected treatment effect are rendered in orange whereas negative correlations
are rendered in blue.](Figs/longitudinalStudy.png)

__Quantitative CT indices.__  Imaging biomarkers for characterizing emphysema in CT have
have been well researched, although there are ample opportunities to refine these methods
as well as to introduce more advanced approaches.  Examples of the latter include texture
analysis for identifying the centrilobular and groundglass opacities and fractal and
connectivity approaches to differentiate centrilobular from panlobular emphysema. The
indices for CT image analysis can roughly be divided into those that characterize the
pulmonary parenchyma:
  volumetric tissue (e.g., [@Coxson:1999aa;@Perez:2005aa]),
   distribution of low attenuation areas (LAA) (e.g., [@Coxson:2005aa;@Stolk:2007aa]),
   cooccurrence and run-length matrix features (e.g., [@Uppaluri:1999aa;@Xu:2006aa]),
   attenuation statistics (e.g., [@Gevenois:1996aa;@Hoffman:2006aa]),
   deformation measures (e.g., [@Gee:2003aa;@Sundaram:2005aa]), and
   stochastic fractal dimension features (e.g., [@Uppaluri:1999aa;@Hoffman:2006aa])
and those that characterize the airways (e.g., [@Aykac:2003aa;@Park:1998aa;@Ederle:2003aa]).

The former are important
for subjects with an emphysematous component of disease, whereas the latter are important
for subjects with a bronchitic component of disease. An important premise of this
proposal is that many of these measurements can also be directly applied to discriminative
analysis using 3He MRI for a variety of lung diseases.
These indices can also be studied not
only at any particular single time point, but also for changes with time. The addition of
quantitative morphologic measurements of the airways provides an assessment of the
contribution of airway changes to chronic lung disease.

\input{ct_table.tex}

Table 1 provides an overview of these types of
discriminative measurements that can be used for CT and 3He lung assessment.
We have already
implemented many of these image features and have contributed the result of our work to
the Insight Toolkit (ITK) of the National Institutes of Health (e.g., [@Tustison:2008aa;@Tustison:2009aa]).
As an
open-source repository for medical image analysis algorithms, contribution of our work
to the ITK allows researchers full access to the latest image analysis algorithms in
addition to avoiding research redundancy. It is also beneficial in that the entire ITK
community participates in the vetting of the software library.


__Airway and vessel segmentation.__  In describing the quantitative CT lung indices,
it was pointed out that lung airway morphology has been previously utilized as a biomarker
for disease characterization.  Additionally, there are other potential uses motivating
the inclusion of airway segmentation in any pulmonary image analysis toolkit (cf Figure 5).
In an evaluation of 15 airway segmentation algorithms [@Lo:2012aa]
it was shown that no algorithm was capable of ``extracting more than 77% of the reference.''
Our plan is to initially provide an implementation of the algorithm developed by our group [@Song:2010aa].
Instead of mixing airway segmentation and leakage detection at every iteration, this work divides
this problem into a hypothesis generation of thin airway paths and a post processing
procedure of removing leakage path candidates. For the purpose of generating as many
hypotheses as possible, a novel speed function for thin airways is used. To exclude
leakage regions, a novel cost function defined on the whole path candidate is used.
Such a scheme is more flexible when evaluating the whole path and can be viewed as
complementary to current region growing methods.

![Potential clinical use case for identifying the feeding airway branch path to the
ventilation defect.  The functional ventilation image is normalized to the corresponding
CT image.  The airways are segmented in the individual subject space.  After identification
of the ventilation defect of interest, we can automatically determine the bronchiole
pathway from the trachea to the defect.](Figs/airways.png)

__Nodule detection.__ CT is used for screening of lung cancers (i.e., pulmonary nodules)
which currently requires human intervention for the laborious and tedius task of manual
scanning.  Automated detection methods could potentially save much time and effort which
has inspired much research literature on the topic including several commercial systems
and specialized visualization hardware for facilitating detection.  In 2009, a nodule
detection competition was held for comparing performance of individual algorithms as well
as their combinations [@Ginneken:2010aa].  This competition included entries from both
academic institutions as well as a system from Philips (although, to our knowledge, none
are available for public use).





### 3(c.3) __Specific Aim 2.__  To provide multiple sets of multi-modal annotated lung data (CT, proton, and 3He) for public use.


__3(c.3.7) Software engineering.__ Both ANTs and ITK-SNAP development utilizes open-source
software engineering best practices, such as the use of Git version management software
for collaborative development and easy branching and merging; use of a centralized repository
(SourceForge) for code, executable and data sharing; use of the CMake/CTest/CDash suite for
cross-platform development, testing and automatic builds. Virtual machines with different
versions of Windows, MacOS and Linux operating systems generate nightly builds and execute
test code, uploading a binary to the central SourceForge repository. ANTs and ITK-SNAP are
documented through video and text tutorials, housed online on dedicated websites [@ANTsWebsite;@SnapWebsite].
A similar infrastructure will be developed for the software resources proposed in Aim 1.

### 3(c.4) __Specific Aim 3.__ Evaluate multiple complete studies with input data from multiple investigators to showcase the utility of the tools and data provided with this proposal.

### Anticipated difficulties
Signal intensity in the lungs is poor in areas of low ventilation. COPD and asthma are
obstructive lung diseases which exhibit focal areas of decreased signal intensity on 3He MRI
which are thought to correspond to areas of reduced ventilation. These ventilation defects
severely inhibit our ability to detect the lung boundaries for proper segmentation. Also,
most of the COPD and asthmatic patients will have ventilation defects with the moderate
asthmatics having greater than 1 defect per slice also negatively affecting boundary
delineation. Note that there are similar issues for CT images of severe pathologies.
However, given the shape and intensity prior statistics contained by our 3He MRI and CT
lung templates, it is expected that the templates, in combination with our proposed
segmentation algorithms, will be sufficient to provide a good initialization for subsequent
manual segmentation if they do not yield an adequate segmentation result. The CT data,
which provides excellent contrast between the lung and chest wall, can also be used to
inform the 3He MRI segmentation.






\clearpage

\newpage

# References

