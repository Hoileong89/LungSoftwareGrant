---
output:
  pdf_document:
    fig_caption: true
    latex_engine: xelatex
    keep_tex: yes
header-includes:
   - \usepackage{booktabs}
bibliography: references.bib
csl: national-science-foundation-grant-proposals.csl
fontsize: 11pt
mainfont: Georgia
geometry: margin=0.5in
---

<!-- https://grants.nih.gov/grants/ElectronicReceipt/pdf_guidelines.htm -->


```{r setup, include=FALSE}
knitr::opts_chunk$set( cache=TRUE )
```

\pagenumbering{gobble}

# Specific Aims

The development and proliferation of quantitative image analysis methods have accelerated
research efforts and are having an increasingly significant impact in modern clinical
practice.  Although the research utility of these techniques has been amply demonstrated in
determining longitudinal and groupwise trends, they are also becoming increasingly relevant
in the clinical setting in providing biomarkers for aiding patient diagnoses, monitoring
disease progression, and determining treatment outcomes.  Increases in the capabilities
and accessibility of computational facilities and a corresponding sophistication in computational
algorithms have only made such practices more commonplace.

One of the most significant hurdles in adopting more quantitative clinical practices and
exploring additional novel research pathways is the availability of accurate, robust, and
easy-to-use image analysis tools.  Historically, the research and clinical communities (and their overlap)
have significantly benefited from the development and proliferation of imaging-related
analysis packages, particularly those softwares which have been tailored for
specific application domains.  Although several such established packages exist for
neuroimaging research (e.g., FSL, FreeSurfer, AFNI, SPM), no such package exists for
pulmonary imaging analysis.  The primary goal of this proposal is to develop
a robust, open-source image analysis toolkit and dissemination platform
 specifically targeted at the pulmonary research community.

Although methodological research is continually being presented at conferences and
published in various venues, the unfortunate reality is that much of this work exists
strictly in “advertisement” form.  Oftentimes the underlying code is unavailable to other
researchers or is implemented in a limited manner (i.e., strictly as proof-of-concept
software).  Frequently, crucial parameter choices are omitted in the corresponding
publication(s) which makes external implementations difficult.  In addition, the data used
to showcase the proposed methodologies are often private and actual data visualization is
limited to carefully selected snapshots for publication (i.e., advertisement) purposes
which might not be representative of algorithmic performance.  Finally, many of these
analysis methods are patented and/or integrated into proprietary commercial software
packages which severely limits accessibility to researchers.

As a corrective alternative, this proposal will provide an open-source software toolkit
targeted for pulmonary research.  As principal developers of the popular, open-source ANTs
(Advanced Normalization Tools) package, we have extensive experience in the development of
well-written software that has gained much traction in the neuroscience community and propose
to make a similar impact in the pulmonary community with this proposal.  Specifically,
we plan to provide methods for core pulmonary image analysis tasks across multiple modalities, many
of which we have proposed previously in past publications.  These basic tasks include pulmonary image registration,
template building for cross-sectional and longitudinal (i.e., respiratory cycle) analyses,
functional and structural lung image segmentation, and computation of quantitative image
indices as potential imaging biomarkers.  In addition to the software, we will
provide additional data, documentation, and tutorial materials consistent with open-science
principles.   Formally, this proposal is defined by the following specific aims:

* __Specific Aim 1:__  __Develop a set of open-source software tools for CT, proton, and He-3
pulmonary computational analysis.__  These open-source software tools will specifically
target pulmonary image analysis and comprise core application functions such as
inspiratory/expiratory registration for inferring pulmonary kinematics, ventilation-based segmentation,
lung and lobe estimation, airway segmentation, and calculation of clinical indices for
characterization of lung development and pathology.  To maximize usability, much (if not all)
of the actual code will be developed and distributed within the  Insight Toolkit of the
National Library of Medicine.
* __Specific Aim 2:__  __Provide multiple sets of multi-modal annotated lung data (CT, proton,
and He3) for unrestricted public use.__  In addition to the public unavailability of the
algorithms used to produce the results discussed in certain publications, the input and output
data is also typically not available.  Such availability would be invaluable to other
researchers in the community for appropriation for their own purposes including algorithmic
performance assessment and running the proposed prior-based methods requiring annotated
input data.
* __Specific Aim 3:__  __Evaluate and disseminate the developed resources by leveraging
input data from multiple partner investigators.__  This aim will evaluate and refine the
developed methodology within the real-world context of pulmonary research being carried out at
various partner sites \textcolor{red}{(e.g., University of Virginia, University of Pennsylvania, \ldots,
need to add others)}.
We will disseminate the results of the project through open-source distribution of the software,
data and write-ups, online user support, and conduct of hands-on training workshops.

\newpage

\begin{center}
{\LARGE \bf ITK-Lung:  A Software Framework for Lung Image Processing and Analysis}
\end{center}


# Research Strategy

## __3(a) Significance__

<!--
### 3(a.1) The importance of pulmonary image analysis tools for research and clinical investigation
-->

The increased utilization of imaging for both research and clinical purposes has  furthered
the demand for quantitative image analysis techniques.  The use of these computational
techniques are motivated by the need for less subjectivity and more standardization in
medical image interpretation, increased speed and automation in diagnosis, and greater
robustness and accuracy for determining biological correlates with imaging findings.
For example, in the area of pharmaceutical development and testing, imaging biomarkers are crucial.
In order to determine fundamental study parameters such as drug safety
and effectiveness, quantitative assessments derived from imaging measures must be objective
and reproducible [@Wang:2010aa] which is often difficult without computational aid given the intra-
and inter-reader variability in radiological practice [@Zhao:2013aa;@McErlean:2013aa].
Additionally, the exciting possibilities associated with "big data" and the potential
for improvement in individualized, evidence-based medicine has also increased the need
for sophisticated data transformation and machine learning techniques.

Well-vetted and publicly available software is a significant benefit
to targeted research communities.  For example, the neuroscience community has greatly
benefited from highly evolved software packages such as FreeSurfer [@Fischl:2012aa], the FMRIB Software
Library (FSL) [@Jenkinson:2012aa], the Analysis of Functional NeuroImages (AFNI) package [@Cox:2012aa], and the
Statistical Parametric Mapping (SPM) package [@Ashburner:2012aa].  Performing a pubmed query for any one of
these softwares every year for the past decade (cf Figure 1) illustrates the growing use of
such packages and the research studies that are produced as a result.  However, despite the
absolute number of articles produced using such software and the year-by-year usage increase,
no such analogous set of tools exist for pulmonary-specific research.  In fact, in a recent
review of CT- and MRI-derived biomarkers for pulmonary clinical investigation, the authorial
consensus is that ``universally available image analysis software'' is a major hinderance
to more widespread usage of such imaging biomarkers [@Hoffman:2015aa].

```{r pubmedQuery, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE, results="hide", fig.height=3, fig.width=6, fig.cap="Number of articles per year which cite publicly available neuroimaging analysis packages (specifically, FreeSurfer, AFNI, FSL, and SPM).  Although the benefits seem clear for the neuroscience community, analogous efforts within the pulmonary community have yet to be undertaken despite consensus amongst researchers and clinicians regarding the utility of such offerings." }
years <- 2005:2014

numberOfAbstractsPerYear <- rep( 0, length( years ) )
for( i in 1:length( years ) )
  {
  cat( "Doing year", years[i], "\n" )
  query = "afni OR fsl OR freesurfer OR (statistical AND parametric AND mapping)"
  res <- EUtilsSummary( query, db = "pubmed", retmax = 5000, mindate = as.character( years[i] ), maxdate = as.character( years[i] ) )
  summary( res )
  fetch <- EUtilsGet( res )
  abstracts <- AbstractText( fetch )
  numberOfAbstractsPerYear[i] <- length( abstracts )
  }

plotDataFrame <- data.frame( Year = as.factor( years ), NumberOfAbstracts = numberOfAbstractsPerYear )
ggplot( data = plotDataFrame, aes( x = Year, y = NumberOfAbstracts, fill = NumberOfAbstracts ) ) +
          geom_bar( stat = "identity" ) +
          scale_fill_gradient( low = "#01256e", high = "#6d0e0e" ) +
          ylab( "Number of articles" ) +
          xlab( "Publication year" ) +
          theme( legend.position = "none" )
```

Medical image analysis libraries (e.g., the NIH-sponsored Insight ToolKit) provide extensive algorithmic
capabilities for a range of generic image processing tasks.  However,
tailored software packages for certain application domains (e.g., lung image analysis) are not
available despite the vast number of algorithms that have been proposed in the literature.
Note that the goals of this proposal would
significantly support the National Library of Medicine’s own open-source directives in that
all software would be developed using the established Insight ToolKit’s coding and testing
standards with the eventual idea that much (if not all) of the actual code would be contributed
for inclusion in future versions of the Insight ToolKit as we have done in the past.
It should also be noted that open-source software, in general, has documented benefits
within the targeted communities for which it is developed and supported.  In addition to
the increase in research output illustrated earlier, open-source permits students and
researchers to learn specific computational techniques in a social environment [@Yunwen:2003aa].
This, in turn, provides motivation for user-based support including potential contributions
such as bug fixes and feature additions.  Additional analyses have shown the tremendous
cost savings that open-source software yields [@Rothwell:2008aa].  Furthermore, it should
be highlighted that open-source development and distribution within a large, and well-invested
community (such as ITK) takes advantage of Linus's law, i.e., ``given enough eyeballs, all bugs are
shallow," for producing robust software.

## __3(b) Innovation__

### 3(b.1) Open source pulmonary algorithmic innovation
Given the lack of open-source solutions for pulmonary image analysis,
the proposal goals would produce an innovative platform for performing such research.
Similar to the brain-specific algorithms provided in our ANTs toolkit, our novel
and useful proposal would include the most essential algorithms for analyzing lung
images from different modalities including CT, 3He, and proton MRI.
Many algorithms have been proposed in various technical venues but that which we propose
would provide well-vetted and easy-to-use implementations of specific robust methodologies
for pulmonary medical image analysis, many of which have been developed by our group.
To facilitate the usage of these algorithms, we will provide several self-contained
online examples (complete with data).

### 3(b.2) Publicly available multi-site data as a reproducible and didactic component

An additional innovative component we are proposing is the inclusion of complete study
data and detailed instructions for generating reproducible, multimodality pulmonary studies
using the proposed package with
input data from several of our external collaborators and colleagues.  Specifically, we have
asked several scientists and researchers who are familiar with our work to provide imaging data
of various modalities which we will then process using the proposed toolkit.  These processed
data will then not only be returned to the corresponding providers with detailed instructions
on reproducing these results in their own labs but will also be provided to the public for any
interested researcher to reproduce the results.  Given the different image acquisition sources,
this strategy should also demonstrate the robustness of our tools.

Included in these analyses will be analyses of our own data.  Any clinical findings
of interest will be published in traditional venues (e.g., Chest).  In addition, we will
provide all image
data and the quantitative analysis scripts as a companion release to accompany the paper
(e.g., see previous similar offerings from our group [@Tustison:2013ac;@Tustison:2014ab]).
 Such a comprehensive clinical
investigation using these tools will not only provide insight into the specifics of certain
pulmonary pathologies
but will also provide a reproducible mechanism for using the tools created with this proposal.

## __3(c) Research design__

### 3(c.1) Preliminary data

### 3(c.1.1) Generic ANTs core tools for image analysis and processing

Many of the core programs comprising portions of the proposed pulmonary software framework
have been created and made available within ANTs.  However, as mentioned earlier, these programs have more
general application and require pulmonary-specific tuning for the pipelines targeted by
this proposal.  The following list comprises these core software tools for
tuning, subsequent extensions, documentation, tutorial generation, and the creation of
easy-to-use bash scripts for large-scale processing of pulmonary imaging data:

![Core processing tools that have made the ANTs package one of the most popular neuroimaging
toolkits.  Fundamental processing tasks such as image registration, template generation,
bias correction, denoising, intensity-based segmentation, and joint label
fusion are extremely well-performing software components which have been utilized for
neuroimaging tasks such as brain extraction and cortical thickness estimation.](Figs/coreANtsToolsNeuro.png)

__ANTs image registration.__  One of the most important methodological developments in
medical image analysis is the advent of image registration techniques capable of
accommodating the highly complex inter-individual variations seen in human neuroanatomy.
Our team is well-recognized for seminal contributions to the field that date back to the
original elastic matching method of Bajcsy and co-investigators
[@Bajcsy:1982aa;@Bajcsy:1989aa;@Gee:1993aa]. Our most recent work, embodied in the ANTs
open-source, cross-platform toolkit for multiple modality image processing,
continues to set the standard in the field. ANTs not only encodes the most advanced results
in registration research, notably the Symmetric Normalization (SyN) algorithm for
diffeomorphisms [@Avants:2008aa], but also packages these within a full featured platform that
includes an extensive library of similarity measures, transformation types, and regularizers.
Recently, a thorough comparison with the original SyN algorithm was performed using a B-spline variant [@Tustison:2013ac].
This evaluation utilized multiple publicly available, annotated brain data sets and
demonstrated statistically significant improvement in label overlap measures.  As part of
that study, we produced the scripts ``antsRegistrationSyN.sh`` and
``antsRegistrationSyNQuick.sh`` which provide a simple interface to our normalization tools
for brain-specific normalization.


__Multi-modal template generation.__  Given the variability in anatomical shape across
populations and the lack of publicly available atlases for specific organs, generating population- or
subject-specific optimal shape/intensity templates significantly enhances study potential
[@Avants:2010aa;@Tustison:2014aa].  First, an average template is estimated via a voxel-wise
mean of all the individual subject
images.  This estimate is iteratively updated by registering each image to the current
template, performing a voxelwise average to create a new estimate, and then "reshaping"
this template based on the average inverse transformation which "moves" the template estimate
closer to the group mean.  See Figure 2 for a cohort-specific multi-modal brain template
for females in the age range 50--60.
This functionality has proven to be a vital component of the ANTs
toolkit for performing neuroimaging research
(e.g., [@Avants:2015aa;@Datta:2012aa;@McMillan:2014aa;@Cook:2014aa;@Tustison2014ad;@Tustison:2014ab]).

__Bayesian-based segmentation with spatial and MRF priors.__
Early statistically-based segmentation work appropriated NASA satellite image processing
software for classification of head tissues in 2-D MR images [@Vannier:1985aa]. Following
this work, many researchers adopted statistical methods for $n$-tissue anatomical brain
segmentation. The Expectation-Maximization (EM) framework is natural [@Dempster:1977aa]
given the "missing data" aspect of this problem. The work described in [@Wells:1996aa]
was one of the first to use EM for finding a locally optimal solution by iterating between
bias field estimation and tissue segmentation.  Core components of this type of work is
the explicit modeling of the tissue intensity values as statistical distributions
[@Cline:1990aa;@Kikinis:1992aa] and the use of MRF modeling [@Geman:1984aa] for regularizing
the classification results [@Held:1997aa].  Recently, reseachers have begun to rely on spatial
prior probability maps of anatomical structures of interest to encode domain knowledge
[@Van-Leemput:1999aa;@Ashburner:2005aa] by providing spatial
prior probability maps and an initial segmentation.  Although this particular segmentation
framework has significant application in the neuroimaging domain, it has also applicable
to other domains such as breast MRI [@Gubern-Merida:2015aa;@Ribes:2014aa] and functional
ventilation of the lung [@Tustison:2011aa].  However, despite the numerous algorithms and other developments which have been proposed
over the years, there are an extremely limited number of software implementations to
perform these types of segmentations.
This deficit inspired us to create our own Bayesian-based
segmentation framework [@Avants:2011aa] (denoted as Atropos) which we have made publicly
available within ANTs.

__N4 bias correction.__  Critical to quantitative processing of MRI is the minimization of field
inhomogeneity effects which produce artificial low frequency intensity variation across the image.
Large-scale studies, such as ADNI, employ perhaps the most widely used bias correction algorithm,
N3 [@Sled:1998aa], as part of their standard protocol [@Boyes:2008aa].
In [@Tustison:2010aa] we introduced an improvement of N3, denoted as "N4", which demonstrates
a significant increase in performance and convergence behavior on a variety of data. This
improvement is a result of an enhanced fitting routine (which includes multi-resolution
capabilities) and a modified optimization formulation.

__Joint label fusion for prior-based segmentation.__  Joint label fusion (JLF) is the current
state-of-the-art for propagating expert labelings from a reference atlas library onto new
instances of unlabeled data. Image registration is used to align the atlas library
(images + segmentations) to a common space. A statistical model is then used to combine
the "guesses" from all the normalized atlas labels to provide a "best guess" estimate of
the target labeling. Several such algorithms have been developed and much effort has been
devoted to determining relative performance levels. See, for example, the recent MICCAI 2012
Grand Challenge and Workshop on Multi-Atlas Labeling).  The joint fusion (JLF) algorithm of
[@Wang:2013aa;@Wang:2013ab] is one of the top performing JLF algorithms. JLF is capable
of predicting anatomical labels with accuracy that rivals expert anatomists [@Yushkevich:2010aa].
It has proven its effectiveness in lung [@Tustison:2015aa], cardiac data [@MALF],
the human brain [@Tustison:2014ab], and in multiple modality canine MRI [@MALF].


__Spatially adaptive denoising.__ Patch-based
denoising is critical for data "cleaning" prior to subsequent processing such
as segmentation or spatial normalization.  Recently, a spatially adaptive approach to denoising
was proposed in [@Manjon:2010aa] which we implemented in ANTs.  This filter
performs well and is also relatively
fast.

The previously described core tools, as well as several others, have been part of ANTs
development efforts for the past decade.  And it was precisely the deficiency of publicly
available tools within the neuroscience community that motivated the inception and
continued development of ANTs.  As a result, our team is well-recognized for our many
open-source contributions including seminal contributions to the
field of image registration outlined earlier.  Indeed, ANTs-based image registration serves
as the basis for the registration component of the latest version of the National Library of
Medicine Insight Toolkit (ITK) programming library (http://www.itk.org). The
combination of state-of-the-art algorithms and feature-rich flexibility has translated
to top-placed rankings in major independent evaluations for certain elements of the ANTs
toolkit:

* SyN was a top performer in a fairly recent large-scale brain normalization evaluation [@Klein:2009aa].
* SyN also competed in the Evaluation of Methods for Pulmonary Image
REgistration 2010 (EMPIRE10) challenge [@Murphy:2011aa] where it was the top performer
for the benchmarks
used to assess lung registration accuracy and biological plausibility of the inferred
transform (i.e., boundary alignment, fissure alignment, landmark correspondence, and
displacement field topology).
* The joint label fusion algorithm of [@Wang:2012aa;@Wang:2013aa] (coupled with SyN)
  performed well in the MICCAI 2012 challenge for labeled brain data [@Landman2012]
  and in 2013 for labeled canine hind leg data [@Asman2013].
* The multivariate template capabilities in ANTs were combined with random forests to win
the Brain Tumor segmentation (BRATS) competition at MICCAI 2013 [@Tustison:2014aa].
* A B-spline variant of the SyN algorithm [@Tustison:2013ac] won the best paper award at the
STACOM 2014 workshop for cardiac motion estimation [@Tustison:2015ab].


### 3(c.1.2) ANTs and the neuroimaging community


ANTs takes advantage of
the mature Insight ToolKit in providing an optimal software framework for building scripts
and programs specifically for neuroimaging.  For example, the following core neuroimage
processing algorithms have been made available through our ANTs toolkit (complete with
online self-contained examples with developer-tuned parameters) and have been used extensively by our group and
others:

* brain normalization [@Avants:2011ab;@Avants:2014aa] (https://github.com/stnava/BasicBrainMapping),
* brain template generation [@Avants:2010aa] (https://github.com/ntustison/TemplateBuildingExample),
* skull-stripping or brain extraction [@Avants:2010ab;@Tustison:2014ab] (https://github.com/ntustison/antsBrainExtractionExample),
* prior-based brain tissue segmentation [@Avants:2011ab] (https://github.com/ntustison/antsAtroposN4Example),
* cortical  thickness estimation [@Das:2009aa;@Tustison:2014ab] (https://github.com/ntustison/antsCorticalThicknessExample),
* brain tumor segmentation [@Tustison:2014aa] (https://github.com/ntustison/ANTsAndArboles), and
* cortical labeling [@Wang:2012aa;@Wang:2013aa] (https://github.com/ntustison/MalfLabelingExample).

All of these tools have been wrapped in easy-to-use, well-documented shell scripts.  For
example, the ANTs cortical thickness pipeline, as outlined in [@Tustison:2014ab], comprises
four major steps:  (1) bias correction, (2) brain extraction, (3) $n$-tissue segmentation,
and (4) cortical thickness estimation.  Each step requires its own set of ANTs tools with
appropriately tuned parameters. To maximize the utility of the pipeline for the interested
user, in [@Tustison:2014ab] we provide all the necessary programs (properly tuned) with
a minimal set of input data required to obtain good results for common data.  The result
is an easy-to-use script that can be invoked by the programmer and non-programmer alike to
obtain the desired processed data which outperforms the current state-of-the-art.  This is
an example command call for the ANTs cortical thickness pipeline:
```bash
  # ANTs processing call for a single subject

  $ sh antsCorticalThickness.sh -d 3 \
                                -a IXI/T1/IXI002-Guys-0828-T1.nii.gz \
                                -e IXI/template/T_Template0.nii.gz \
                                -m IXI/template/T_template0ProbabilityMask.nii.gz \
                                -f IXI/template/T_template0ExtractionMask.nii.gz \
                                -p IXI/template/Priors/priors%d.nii.gz \
                                -o IXI/ANTsResults/IXI002-Guys-02828-
```
This approach to reducing the steep learning curve associated with many processing pipelines
has several benefits.  Bash is an extremely common command language that permits large-scale
processing.  Thus, running several jobs on a cluster infrastructure is straightforward with
this approach (as opposed to a GUI-driven processing paradigm).  Such scripts are readable
by the interested user who can glean parameters as well as manually make changes.

### 3(c.1.3) The significance of ANTs for the pulmonary imaging community


 <!--
\textcolor{red}{Similarly, we used the EMPIRE10 challenge framework to provide an additional comparison in
the context of pulmonary CT image registration [@Tustison:2012aa].  Registration accuracy
is based on a combination of factors including lung boundary and fissure overlap, landmark
correspondence, and topology considerations in the displacement field.  In this domain, the
B-spline variant showed a separate performance gain and has since become
the preferred transformation model for small deformation image registration problems
(an additional domain is cardiac MRI where it recently won the best paper
award [@Tustison:2015ab]).  As part of the development, we will provide simple script-based
interfaces for lung-specific normalization tasks.}
-->


Analogously, several algorithmic categories exist for lung image analysis which, as we have
stated previously, do not exist in any comprehensive, publicly available package.  An
extensive survey concentrating on the years 1999–2004 is given in [@Sluimer:2006aa] which
covers computer
aided diagnosis of lung disease and lung cancer in CT (i.e., detection and tracking of
pulmonary nodules) and provides an overview of the many relevant segmentation methods for
pulmonary structures. Although many algorithms existed at the time, continued technical
development has only increased the number of available algorithms.  However, despite the
continued _reporting_ of pulmonary image analysis algorithms, there is no corresponding
increase in algorithmic _availability_. The following is a small sampling of more recently
reported techniques for CT analysis:

* whole lung differentiation from the chest wall (e.g., [@De-Nunzio:2011aa;@Prasad:2008aa;@Wang:2009aa;@Rikxoort:2009aa])
* bronchial structure extraction (e.g., [@Zheng:2007aa;@Nakamura:2008aa] and the many submissions to the recent Extraction of Airways from CT (ExACT) challenge of the 2nd International Workshop on Pulmonary Image Analysis [@Lo:2009aa]),
* vasculature segmentation (e.g., [@Agam:2005aa;@Korfiatis:2011aa]),
* lobe and/or fissure detection (e.g., [@Qi:2014aa;@Doel:2015aa]),
* feature extraction and classification (e.g., [@Uppaluri:1999aa;@Rosas:2011aa;@DeBoer:2014aa]), and
* nodule detection (e.g., [@Messay:2010aa;] and the many submissions to the Automatic Nodule Detection (ANODE09) challenge of the 2009 CAD Conference of SPIE Medical Imaging [@Ginneken:2010aa]).

Since this list is restricted to CT image analysis, inclusion of additional techniques
specific to other modalities will have additional benefit.  For example, ventilation-based
segmentation for analysis of ventilation lung imaging (e.g., [@Tustison:2011aa] which was
implemented within the ANTs framework) will also have significant impact in a comprehensive
lung image analysis suite.  Since most of the tools that have been developed within the
ANTs framework have generic applicability, crucial to the development of our proposed toolset
will be domain-specific experience.  For example, although ANTs performance in brain
registration has been independently evaluated and found to be of relatively high quality
[Klein2009], tailoring our registration tool in the EMPIRE10 challenge (Evaluation of Methods
for Pulmonary Image REgistration 2010) required significant empirically-based tuning.  In
addition, new innovations in diffeomorphic registration technology has led to a Symmetric
Normalization B-spline variant which has demonstrated accurate normalizations [@Tustison:2013ac],
and transformations which are particularly well-suited for pulmonary data [@Tustison:2015aa].













Although the template
construction algorithm described in [@Avants:2010aa] was applied to T1-weighted brain data
(with the extension of multimodal data described in [@Tustison:2014aa]), it is sufficiently
generic such that it can be applied to pulmonary data.



For example, in Fig. 4, we illustrate the major processing components of a recent study
analyzing local changes based on a pulmonary treatment plan [@Tustison:2013ad].  This
study employed several of the tools we are proposing for inclusion in the specified aims.
The firs major component is the construction of a single-subject 3He/proton MRI
template for all five imaging time points.  This step generates the statistical coordinate
system for the voxelwise regression analysis of the normalized intensities to determine
correlation with expected treatment effects.

![Voxelwise regression analysis to determine image-based response to treatment.  First,
a multi-modal, single-subject template is created to bring all time point images to
the same coordinate system.  4-D segmentation is performed on the longitudinal time series
of 3-D image volumes.  Treatment
effects are expected to follow the simplified treatment hypothesis illustrated with the
dashed blue line in the plot on the right.  To explore how the longitudinal change in expected
ventilation follows this treatment hypothesis with image data, we smooth the aligned expected
ventilation maps (to account for potential voxelwise misalignments) and then quantify how the
voxelwise intensities regress with the simplified treatment hypothesis.  This quantification
is visualized using the correlation maps depicted in the template space (top right).  Positive
correlations with the expected treatment effect are rendered in orange whereas negative correlations
are rendered in blue.](Figs/longitudinalStudy.png)









__Atlas-based lung segmentation.__  Identification of anatomical structure in MRI is often a
crucial preprocessing step for quantification of morphological features or ventilation
information from functional images.  Quantitative regional analysis often requires the
identification of lung and lobar anatomy.  Although much algorithmic research for lung
segmentation has been
reported in the CT literature [@Rikxoort:2013aa], co-opting such technologies is complicated by
MRI-specific issues such as RF coil inhomogeneity, presence and resolution of structural detail, and
the absence of a physically-based intensity scaling.

We recently proposed a multi-atlas approach for automatically segmenting the left and right
lungs in proton MRI [@Tustison:2015aa].  Multi-atlas approaches to segmentation have proven highly
successful in neuroimaging [@Wang:2012aa;@Wang:2013aa] and these methods translate readily to
the pulmonary domain.  Wherease many current strategies for lung image segmentation employ
low-level processing techniques based on encodable heuristics, consensus-based strategies,
in contrast, optimize the prior knowledge applied to a specific segmentation problem (cf Figure 3).
The evaluation of our proposed method [@Tustison:2015aa] demonstrated good performance
with Jaccard overlap measures for the left and right lungs being $0.966 \pm 0.018$ and
$0.970 \pm 0.016$, respectively.  One of the benefits of this approach is that can also
be applied effectively to pulmonary CT.

![Sample lung and lobe estimation results in both proton MRI and CT using our
atlas-based strategy.  (Left) Lung segmentation and lobe estimation results for the given
proton MRI.  Although lobe estimation is dependent solely on the warped atlases, we are
able to obtain accurate estimates of lobes which are useful for more regional analysis
and provide a more intuitive and universal subdivision of the lungs than previous partitioning
schemes.  (Right) The utility of this method extends to CT where the integrity of lobar anatomical
markers (such as the lack of fissures illustrated by the red arrows) have been compromised due to
disease.](Figs/lungEstimation.png)

__Atlas-based lobe estimation.__  For regional investigation of certain lung pathologies and
conditions, it is often useful to quantify measurements of interest within more localized
regions, such as the lobes.  However there is little (if any) usable information in proton
MRI for image-based lobar segmentation which has led to alternative geometric subdivisions
which are ad hoc, non-anatomical, and do not adequately address intra- and inter-subject
correspondences.  However, we can take advantage of
inter-subject similarities in lobar geometry to provide a prior-based estimation of
lobar divisions using a consensus labeling approach (cf Figure 3).

To generate the lobe segmentation in a target proton or CT lung image, we first generate the
binary whole lung mask using the whole lung atlas-based estimation.  We
then register the set of CT lung masks which have beeen expertly annotated to the target
binary lung mask using the B-spline
SyN registration approach described earlier [@Tustison:2013ac].
 Subsequently, we warp the set of CT lobe labels to the target image using
the CT mask-to-target mask transformation.  Since we have no intensity information inside
the target lung mask and CT atlas lung masks, we use a simply majority voting strategy to
generate the optimal labeling for the target image.  Following the majority voting, we
remove any labelings outside the lung mask and assign any unlabeled voxels with the label
closest in distance to that voxel.  This methodology is more thoroughly described in
[@Tustison:2015aa] where we showed that lobar overlap measures in proton MRI were on par
with the  state-of-the-art CT methods where fissure information is actually visible
(left upper: $0.882 \pm 0.059$, left lower: $0.868 \pm 0.06$, right upper: $0.852 \pm 0.067$,
right middle: $0.657 \pm 0.130$, right lower: $0.873 \pm 0.063$).

__Ventilation quantification.__
Automated or semiautomated approaches for classifying areas of varying degrees of ventilation
are of potential benefit for pulmonary functional analysis.  In [@Tustison:2011aa], we presented
an automated algorithmic pipeline for ventilation-based partitioning of the lungs in hyperpolarized
3He and 129Xe MRI.  Without ground truth data for evaluation, we used a consensus labeling approach [@Warfield:2004aa]
to simultaneously estimate the true segmentation from given ``raters''
which included the segmentation from our automated approach and the manual tracings of three trained
individuals.  In terms of combined specificity and sensitivity, our automated algorithm
demonstrated superior performance with the added benefit of being reproducible and less
time-consuming.

Since the initial development, we have continued to improve this segmentation pipeline by
incorporating an iterative bias-correction/segmetnation estimation scheme.   An additional
component that improves results is an ANTs-based implementation of the patch-based denoising protocol
described in [@Manjon:2010aa].  Example longitudinal segmentation results are provided
in Figure 4.

![Pulmonary functional segmentation using the algorithmic framework first described in [@Tustison:2011aa]
for hyperpolarized 3He MRI.  These data were taken from a current study looking at the
implications in ventilation pre- and post-albuterol intake including an additional
acquisition at some delay period following the post-albuterol imaging.  The
ventilation-based segmentation is as follows:  red = no ventilation, green = poorly
ventilated, blue = normally ventilated, and yellow = well-ventilated.
Note the improvement in both the qualitative assessment of the ventilation map (top) and the corresponding
segmentation time course (bottom) followed by an approximate return to pre-albuterol
conditions following the delay period.](Figs/prePostAlbuterol.png)
















### 3(c.2) Specific Aim 1:   To develop a set of open-source software tools for CT, proton, and 3He pulmonary computational analysis.


__Quantitative CT indices.__  Imaging biomarkers for characterizing emphysema in CT have
have been well researched, although there are ample opportunities to refine these methods
as well as to introduce more advanced approaches.  Examples of the latter include texture
analysis for identifying the centrilobular and groundglass opacities and fractal and
connectivity approaches to differentiate centrilobular from panlobular emphysema. The
indices for CT image analysis can roughly be divided into those that characterize the
pulmonary parenchyma:
  volumetric tissue (e.g., [@Coxson:1999aa;@Perez:2005aa]),
   distribution of low attenuation areas (LAA) (e.g., [@Coxson:2005aa;@Stolk:2007aa]),
   cooccurrence and run-length matrix features (e.g., [@Uppaluri:1999aa;@Xu:2006aa]),
   attenuation statistics (e.g., [@Gevenois:1996aa;@Hoffman:2006aa]),
   deformation measures (e.g., [@Gee:2003aa;@Sundaram:2005aa]), and
   stochastic fractal dimension features (e.g., [@Uppaluri:1999aa;@Hoffman:2006aa])
and those that characterize the airways (e.g., [@Aykac:2003aa;@Park:1998aa;@Ederle:2003aa]).

The former are important
for subjects with an emphysematous component of disease, whereas the latter are important
for subjects with a bronchitic component of disease. An important premise of this
proposal is that many of these measurements can also be directly applied to discriminative
analysis using 3He MRI for a variety of lung diseases.
These indices can also be studied not
only at any particular single time point, but also for changes with time. The addition of
quantitative morphologic measurements of the airways provides an assessment of the
contribution of airway changes to chronic lung disease.

\input{ct_table.tex}

Table 1 provides an overview of these types of
discriminative measurements that can be used for CT and 3He lung assessment.
We have already
implemented many of these image features and have contributed the result of our work to
the Insight Toolkit (ITK) of the National Institutes of Health (e.g., [@Tustison:2008aa;@Tustison:2009aa]). As an
open-source repository for medical image analysis algorithms, contribution of our work
to the ITK allows researchers full access to the latest image analysis algorithms in
addition to avoiding research redundancy. It is also beneficial in that the entire ITK
community participates in the vetting of the software library.

__Airway and vessel segmentation.__  In describing the quantitative CT lung indices,
it was pointed out that lung airway morphology has been previously utilized as a biomarker
for disease characterization.  Additionally, there are other potential uses motivating
the inclusion of airway segmentation in any pulmonary image analysis toolkit (cf Figure 5).
In an evaluation of 15 airway segmentation algorithms [@Lo:2012aa]
it was shown that no algorithm was capable of ``extracting more than 77% of the reference.''
Our plan is to initially provide an implementation of the algorithm developed by our group [@Song:2010aa].
Instead of mixing airway segmentation and leakage detection at every iteration, this work divides
this problem into a hypothesis generation of thin airway paths and a post processing
procedure of removing leakage path candidates. For the purpose of generating as many
hypotheses as possible, a novel speed function for thin airways is used. To exclude
leakage regions, a novel cost function defined on the whole path candidate is used.
Such a scheme is more flexible when evaluating the whole path and can be viewed as
complementary to current region growing methods.

![Potential clinical use case for identifying the feeding airway branch path to the
ventilation defect.  The functional ventilation image is normalized to the corresponding
CT image.  The airways are segmented in the individual subject space.  After identification
of the ventilation defect of interest, we can automatically determine the bronchiole
pathway from the trachea to the defect.](Figs/airways.png)

__Nodule detection.__





__Specific Aim 2.__  To provide multiple sets of multi-modal annotated lung data (CT, proton, and He3) for public use.

__Specific Aim 3.__ Evaluate and disseminate multiple complete studies with input data from
multiple investigators to showcase the utility of the tools and data provided with this proposal.

### Anticipated difficulties
Signal intensity in the lungs is poor in areas of low ventilation. COPD and asthma are
obstructive lung diseases which exhibit focal areas of decreased signal intensity on 3HeMRI
which are thought to correspond to areas of reduced ventilation. These ventilation defects
severely inhibit our ability to detect the lung boundaries for proper segmentation. Also,
most of the COPD and asthmatic patients will have ventilation defects with the moderate
asthmatics having greater than 1 defect per slice also negatively affecting boundary
delineation. Note that there are similar issues for CT images of severe pathologies.
However, given the shape and intensity prior statistics contained by our 3HeMRI and CT
lung templates, it is expected that the templates, in combination with our proposed
segmentation algorithms, will be sufficient to provide a good initialization for subsequent
manual segmentation if they do not yield an adequate segmentation result. The CT data,
which provides excellent contrast between the lung and chest wall, can also be used to
inform the 3HeMRI segmentation.






\clearpage

\newpage

# References

